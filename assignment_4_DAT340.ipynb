{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BitnooriLee/Applied-Machine-Learning/blob/main/assignment_4_DAT340.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Programming Assignment 4 \n",
        "\n",
        "Bitnoori Lee\n",
        "\n",
        "Sena Bayraktaroglu\n",
        "\n",
        "INTRODUCTION: Experiment Code"
      ],
      "metadata": {
        "id": "wm-y9pnQC8ZI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-n_GCsZGN797",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9efe61a5-4608-42ed-e40b-c437d6c74e9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "# Import Data set\n",
        "from google.colab import drive\n",
        "import csv\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "root = '/content/gdrive/My Drive/DAT340/'\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "#from sklearn.model_selection import train_test_split\n",
        "\n",
        "# the actual classification algorithm\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.naive_bayes import CategoricalNB\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "# for converting training and test datasets into matrices\n",
        "# TfidfVectorizer does this specifically for documents\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# for bundling the vectorizer and the classifier as a single \"package\"\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# for splitting the dataset into training and test sets \n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# for evaluating the quality of the classifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "root_new = root +\"pa4/pa4/data/all_sentiment_shuffled.txt\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\"This file shows a couple of implementations of the perceptron learning\n",
        "algorithm. It is based on the code from Lecture 3, but using the slightly\n",
        "more compact perceptron formulation that we saw in Lecture 6.\n",
        "\n",
        "There are two versions: Perceptron, which uses normal NumPy vectors and\n",
        "matrices, and SparsePerceptron, which uses sparse vectors and matrices.\n",
        "The latter may be faster when we have high-dimensional feature representations\n",
        "with a lot of zeros, such as when we are using a \"bag of words\" representation\n",
        "of documents.\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator\n",
        "\n",
        "class LinearClassifier(BaseEstimator):\n",
        "    \"\"\"\n",
        "    General class for binary linear classifiers. Implements the predict\n",
        "    function, which is the same for all binary linear classifiers. There are\n",
        "    also two utility functions.\n",
        "    \"\"\"\n",
        "\n",
        "    def decision_function(self, X):\n",
        "        \"\"\"\n",
        "        Computes the decision function for the inputs X. The inputs are assumed to be\n",
        "        stored in a matrix, where each row contains the features for one\n",
        "        instance.\n",
        "        \"\"\"\n",
        "        return X.dot(self.w)\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predicts the outputs for the inputs X. The inputs are assumed to be\n",
        "        stored in a matrix, where each row contains the features for one\n",
        "        instance.\n",
        "        \"\"\"\n",
        "\n",
        "        # First compute the output scores\n",
        "        scores = self.decision_function(X)\n",
        "\n",
        "        # Select the positive or negative class label, depending on whether\n",
        "        # the score was positive or negative.\n",
        "        out = np.select([scores >= 0.0, scores < 0.0],\n",
        "                        [self.positive_class,\n",
        "                         self.negative_class])\n",
        "        return out\n",
        "\n",
        "    def find_classes(self, Y):\n",
        "        \"\"\"\n",
        "        Finds the set of output classes in the output part Y of the training set.\n",
        "        If there are exactly two classes, one of them is associated to positive\n",
        "        classifier scores, the other one to negative scores. If the number of\n",
        "        classes is not 2, an error is raised.\n",
        "        \"\"\"\n",
        "        classes = sorted(set(Y))\n",
        "        if len(classes) != 2:\n",
        "            raise Exception(\"this does not seem to be a 2-class problem\")\n",
        "        self.positive_class = classes[1]\n",
        "        self.negative_class = classes[0]\n",
        "\n",
        "    def encode_outputs(self, Y):\n",
        "        \"\"\"\n",
        "        A helper function that converts all outputs to +1 or -1.\n",
        "        \"\"\"\n",
        "        return np.array([1 if y == self.positive_class else -1 for y in Y])\n",
        "\n",
        "\n",
        "class Perceptron(LinearClassifier):\n",
        "    \"\"\"\n",
        "    A straightforward implementation of the perceptron learning algorithm.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_iter=20):\n",
        "        \"\"\"\n",
        "        The constructor can optionally take a parameter n_iter specifying how\n",
        "        many times we want to iterate through the training set.\n",
        "        \"\"\"\n",
        "        self.n_iter = n_iter\n",
        "\n",
        "    def fit(self, X, Y):\n",
        "        \"\"\"\n",
        "        Train a linear classifier using the perceptron learning algorithm.\n",
        "        \"\"\"\n",
        "\n",
        "        # First determine which output class will be associated with positive\n",
        "        # and negative scores, respectively.\n",
        "        self.find_classes(Y)\n",
        "\n",
        "        # Convert all outputs to +1 (for the positive class) or -1 (negative).\n",
        "        Ye = self.encode_outputs(Y)\n",
        "\n",
        "        # If necessary, convert the sparse matrix returned by a vectorizer\n",
        "        # into a normal NumPy matrix.\n",
        "        if not isinstance(X, np.ndarray):\n",
        "            X = X.toarray()\n",
        "\n",
        "        # Initialize the weight vector to all zeros.\n",
        "        n_features = X.shape[1]\n",
        "        self.w = np.zeros(n_features)\n",
        "\n",
        "        # Perceptron algorithm:\n",
        "        for i in range(self.n_iter):\n",
        "            for x, y in zip(X, Ye):\n",
        "\n",
        "                # Compute the output score for this instance.\n",
        "                score = x.dot(self.w)\n",
        "\n",
        "                # If there was an error, update the weights.\n",
        "                if y*score <= 0:\n",
        "                    self.w += y*x\n",
        "\n",
        "\n",
        "##### The following part is for the optional task.\n",
        "\n",
        "### Sparse and dense vectors don't collaborate very well in NumPy/SciPy.\n",
        "### Here are two utility functions that help us carry out some vector\n",
        "### operations that we'll need.\n",
        "\n",
        "def add_sparse_to_dense(x, w, factor):\n",
        "    \"\"\"\n",
        "    Adds a sparse vector x, scaled by some factor, to a dense vector.\n",
        "    This can be seen as the equivalent of w += factor * x when x is a dense\n",
        "    vector.\n",
        "    \"\"\"\n",
        "    w[x.indices] += factor * x.data\n",
        "\n",
        "def sparse_dense_dot(x, w):\n",
        "    \"\"\"\n",
        "    Computes the dot product between a sparse vector x and a dense vector w.\n",
        "    \"\"\"\n",
        "    return np.dot(w[x.indices], x.data)\n",
        "\n",
        "\n",
        "class SparsePerceptron(LinearClassifier):\n",
        "    \"\"\"\n",
        "    A straightforward implementation of the perceptron learning algorithm,\n",
        "    assuming that the input feature matrix X is sparse.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_iter=20):\n",
        "        \"\"\"\n",
        "        The constructor can optionally take a parameter n_iter specifying how\n",
        "        many times we want to iterate through the training set.\n",
        "        \"\"\"\n",
        "        self.n_iter = n_iter\n",
        "\n",
        "    def fit(self, X, Y):\n",
        "        \"\"\"\n",
        "        Train a linear classifier using the perceptron learning algorithm.\n",
        "\n",
        "        Note that this will only work if X is a sparse matrix, such as the\n",
        "        output of a scikit-learn vectorizer.\n",
        "        \"\"\"\n",
        "        self.find_classes(Y)\n",
        "\n",
        "        # First determine which output class will be associated with positive\n",
        "        # and negative scores, respectively.\n",
        "        Ye = self.encode_outputs(Y)\n",
        "\n",
        "        # Initialize the weight vector to all zeros.\n",
        "        self.w = np.zeros(X.shape[1])\n",
        "\n",
        "        # Iteration through sparse matrices can be a bit slow, so we first\n",
        "        # prepare this list to speed up iteration.\n",
        "        XY = list(zip(X, Ye))\n",
        "\n",
        "        for i in range(self.n_iter):\n",
        "            for x, y in XY:\n",
        "\n",
        "                # Compute the output score for this instance.\n",
        "                # (This corresponds to score = x.dot(self.w) above.)\n",
        "                score = sparse_dense_dot(x, self.w)\n",
        "\n",
        "                # If there was an error, update the weights.\n",
        "                if y*score <= 0:\n",
        "                    # (This corresponds to self.w += y*x above.)\n",
        "                    add_sparse_to_dense(x, self.w, y)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lUy97QaZPQRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from aml_perceptron import Perceptron, SparsePerceptron\n",
        "\n",
        "# This function reads the corpus, returns a list of documents, and a list\n",
        "# of their corresponding polarity labels. \n",
        "import time\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "def read_data(corpus_file):\n",
        "    X = []\n",
        "    Y = []\n",
        "    with open(corpus_file, encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            _, y, _, x = line.split(maxsplit=3)\n",
        "            X.append(x.strip())\n",
        "            Y.append(y)\n",
        "    return X, Y\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    # Read all the documents.\n",
        "    X, Y = read_data(root_new)\n",
        "    \n",
        "    # Split into training and test parts.\n",
        "    Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2,\n",
        "                                                    random_state=0)\n",
        "    \n",
        "    # Set up the preprocessing steps and the classifier.\n",
        "    pipeline = make_pipeline(\n",
        "        TfidfVectorizer(),\n",
        "        SelectKBest(k=1000),\n",
        "        Normalizer(),\n",
        "\n",
        "        # NB that this is our Perceptron, not sklearn.linear_model.Perceptron\n",
        "        Perceptron()  \n",
        "    )\n",
        "\n",
        "    # Train the classifier.\n",
        "    t0 = time.time()\n",
        "    pipeline.fit(Xtrain, Ytrain)\n",
        "    t1 = time.time()\n",
        "    print('Training time: {:.2f} sec.'.format(t1-t0))\n",
        "\n",
        "    # Evaluate on the test set.\n",
        "    Yguess = pipeline.predict(Xtest)\n",
        "    print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))"
      ],
      "metadata": {
        "id": "94jr2PuZO1pW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8779d2fa-fde8-4e7b-d351-3f698e263d86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training time: 2.56 sec.\n",
            "Accuracy: 0.7919.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(Y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnZNzVHMQdOg",
        "outputId": "9e4648f0-ffa9-4798-ca10-c9b489706d00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11914\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing the SVC"
      ],
      "metadata": {
        "id": "-myaAH6nCVqm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"This file shows a couple of implementations of the perceptron learning\n",
        "algorithm. It is based on the code from Lecture 3, but using the slightly\n",
        "more compact perceptron formulation that we saw in Lecture 6.\n",
        "\n",
        "There are two versions: Perceptron, which uses normal NumPy vectors and\n",
        "matrices, and SparsePerceptron, which uses sparse vectors and matrices.\n",
        "The latter may be faster when we have high-dimensional feature representations\n",
        "with a lot of zeros, such as when we are using a \"bag of words\" representation\n",
        "of documents.\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator\n",
        "\n",
        "class LinearClassifier(BaseEstimator):\n",
        "    \"\"\"\n",
        "    General class for binary linear classifiers. Implements the predict\n",
        "    function, which is the same for all binary linear classifiers. There are\n",
        "    also two utility functions.\n",
        "    \"\"\"\n",
        "\n",
        "    def decision_function(self, X):\n",
        "        \"\"\"\n",
        "        Computes the decision function for the inputs X. The inputs are assumed to be\n",
        "        stored in a matrix, where each row contains the features for one\n",
        "        instance.\n",
        "        \"\"\"\n",
        "        return X.dot(self.w)\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predicts the outputs for the inputs X. The inputs are assumed to be\n",
        "        stored in a matrix, where each row contains the features for one\n",
        "        instance.\n",
        "        \"\"\"\n",
        "\n",
        "        # First compute the output scores\n",
        "        scores = self.decision_function(X)\n",
        "\n",
        "        # Select the positive or negative class label, depending on whether\n",
        "        # the score was positive or negative.\n",
        "        out = np.select([scores >= 0.0, scores < 0.0],\n",
        "                        [self.positive_class,\n",
        "                         self.negative_class])\n",
        "        return out\n",
        "\n",
        "    def find_classes(self, Y):\n",
        "        \"\"\"\n",
        "        Finds the set of output classes in the output part Y of the training set.\n",
        "        If there are exactly two classes, one of them is associated to positive\n",
        "        classifier scores, the other one to negative scores. If the number of\n",
        "        classes is not 2, an error is raised.\n",
        "        \"\"\"\n",
        "        classes = sorted(set(Y))\n",
        "        if len(classes) != 2:\n",
        "            raise Exception(\"this does not seem to be a 2-class problem\")\n",
        "        self.positive_class = classes[1]\n",
        "        self.negative_class = classes[0]\n",
        "\n",
        "    def encode_outputs(self, Y):\n",
        "        \"\"\"\n",
        "        A helper function that converts all outputs to +1 or -1.\n",
        "        \"\"\"\n",
        "        return np.array([1 if y == self.positive_class else -1 for y in Y])\n",
        "\n",
        "\n",
        "class SVC(LinearClassifier):\n",
        "    \"\"\"\n",
        "    A straightforward implementation of the perceptron learning algorithm.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_iter=20, C = 0.01): # C is regularization parameter\n",
        "        \"\"\"\n",
        "        The constructor can optionally take a parameter n_iter specifying how\n",
        "        many times we want to iterate through the training set.\n",
        "        \"\"\"\n",
        "        self.n_iter = n_iter\n",
        "        self.C = C\n",
        "\n",
        "    def fit(self, X, Y):\n",
        "        \"\"\"\n",
        "        Train a linear classifier using the perceptron learning algorithm.\n",
        "        \"\"\"\n",
        "\n",
        "        # First determine which output class will be associated with positive\n",
        "        # and negative scores, respectively.\n",
        "        self.find_classes(Y)\n",
        "\n",
        "        # Convert all outputs to +1 (for the positive class) or -1 (negative).\n",
        "        Ye = self.encode_outputs(Y)\n",
        "\n",
        "        # If necessary, convert the sparse matrix returned by a vectorizer\n",
        "        # into a normal NumPy matrix.\n",
        "        if not isinstance(X, np.ndarray):\n",
        "            X = X.toarray()\n",
        "\n",
        "        # Initialize the weight vector to all zeros.\n",
        "        n_features = X.shape[1]\n",
        "        self.w = np.zeros(n_features) # w = 000 \n",
        "\n",
        "        # Perceptron algorithm:\n",
        "        for i in range(self.n_iter): # repeat \n",
        "            t = 0 \n",
        "            for x, y in zip(X, Ye): # select  a training pair \n",
        "                t = t +1 \n",
        "                eta = 1/(self.C*t)\n",
        "                # Compute the output score for this instance.\n",
        "                score = x.dot(self.w)\n",
        "\n",
        "                # If there was an error, update the weights.\n",
        "                #if y*score <= 0:\n",
        "                #    self.w += y*x\n",
        "                if y*score < 1:\n",
        "                    self.w =(1-eta*self.C)*self.w + (eta*y)*x\n",
        "                else:\n",
        "                    self.w =(1-eta*self.C)*self.w \n",
        "\n",
        "\n",
        "##### The following part is for the optional task.\n",
        "\n",
        "### Sparse and dense vectors don't collaborate very well in NumPy/SciPy.\n",
        "### Here are two utility functions that help us carry out some vector\n",
        "### operations that we'll need.\n",
        "\n",
        "def add_sparse_to_dense(x, w, factor):\n",
        "    \"\"\"\n",
        "    Adds a sparse vector x, scaled by some factor, to a dense vector.\n",
        "    This can be seen as the equivalent of w += factor * x when x is a dense\n",
        "    vector.\n",
        "    \"\"\"\n",
        "    w[x.indices] += factor * x.data\n",
        "\n",
        "def sparse_dense_dot(x, w):\n",
        "    \"\"\"\n",
        "    Computes the dot product between a sparse vector x and a dense vector w.\n",
        "    \"\"\"\n",
        "    return np.dot(w[x.indices], x.data)\n",
        "\n",
        "\n",
        "class SparsePerceptron(LinearClassifier):\n",
        "    \"\"\"\n",
        "    A straightforward implementation of the perceptron learning algorithm,\n",
        "    assuming that the input feature matrix X is sparse.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_iter=20):\n",
        "        \"\"\"\n",
        "        The constructor can optionally take a parameter n_iter specifying how\n",
        "        many times we want to iterate through the training set.\n",
        "        \"\"\"\n",
        "        self.n_iter = n_iter\n",
        "\n",
        "    def fit(self, X, Y):\n",
        "        \"\"\"\n",
        "        Train a linear classifier using the perceptron learning algorithm.\n",
        "\n",
        "        Note that this will only work if X is a sparse matrix, such as the\n",
        "        output of a scikit-learn vectorizer.\n",
        "        \"\"\"\n",
        "        self.find_classes(Y)\n",
        "\n",
        "        # First determine which output class will be associated with positive\n",
        "        # and negative scores, respectively.\n",
        "        Ye = self.encode_outputs(Y)\n",
        "\n",
        "        # Initialize the weight vector to all zeros.\n",
        "        self.w = np.zeros(X.shape[1])\n",
        "\n",
        "        # Iteration through sparse matrices can be a bit slow, so we first\n",
        "        # prepare this list to speed up iteration.\n",
        "        XY = list(zip(X, Ye))\n",
        "\n",
        "        for i in range(self.n_iter):\n",
        "            for x, y in XY:\n",
        "\n",
        "                # Compute the output score for this instance.\n",
        "                # (This corresponds to score = x.dot(self.w) above.)\n",
        "                score = sparse_dense_dot(x, self.w)\n",
        "\n",
        "                # If there was an error, update the weights.\n",
        "                if y*score <= 0:\n",
        "                    # (This corresponds to self.w += y*x above.)\n",
        "                    add_sparse_to_dense(x, self.w, y)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PWSXDIhYWmyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = make_pipeline(\n",
        "        TfidfVectorizer(),\n",
        "        SelectKBest(k=1000),\n",
        "        Normalizer(),\n",
        "\n",
        "        # NB that this is our Perceptron, not sklearn.linear_model.Perceptron\n",
        "        SVC(C=1/len(Y))  \n",
        "    )\n",
        "\n",
        "# Train the classifier.\n",
        "t0 = time.time()\n",
        "pipeline.fit(Xtrain, Ytrain)\n",
        "t1 = time.time()\n",
        "print('Training time: {:.2f} sec.'.format(t1-t0))\n",
        "\n",
        "# Evaluate on the test set.\n",
        "Yguess = pipeline.predict(Xtest)\n",
        "print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywg77WH1T9uB",
        "outputId": "3656ee44-0278-4f61-a799-eef4c79bf4ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training time: 2.73 sec.\n",
            "Accuracy: 0.8099.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LOGISTIC REGRESSION"
      ],
      "metadata": {
        "id": "PfahaYhaCskK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"This file shows a couple of implementations of the perceptron learning\n",
        "algorithm. It is based on the code from Lecture 3, but using the slightly\n",
        "more compact perceptron formulation that we saw in Lecture 6.\n",
        "\n",
        "There are two versions: Perceptron, which uses normal NumPy vectors and\n",
        "matrices, and SparsePerceptron, which uses sparse vectors and matrices.\n",
        "The latter may be faster when we have high-dimensional feature representations\n",
        "with a lot of zeros, such as when we are using a \"bag of words\" representation\n",
        "of documents.\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator\n",
        "\n",
        "class LinearClassifier(BaseEstimator):\n",
        "    \"\"\"\n",
        "    General class for binary linear classifiers. Implements the predict\n",
        "    function, which is the same for all binary linear classifiers. There are\n",
        "    also two utility functions.\n",
        "    \"\"\"\n",
        "\n",
        "    def decision_function(self, X):\n",
        "        \"\"\"\n",
        "        Computes the decision function for the inputs X. The inputs are assumed to be\n",
        "        stored in a matrix, where each row contains the features for one\n",
        "        instance.\n",
        "        \"\"\"\n",
        "        return X.dot(self.w)\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predicts the outputs for the inputs X. The inputs are assumed to be\n",
        "        stored in a matrix, where each row contains the features for one\n",
        "        instance.\n",
        "        \"\"\"\n",
        "\n",
        "        # First compute the output scores\n",
        "        scores = self.decision_function(X)\n",
        "\n",
        "        # Select the positive or negative class label, depending on whether\n",
        "        # the score was positive or negative.\n",
        "        out = np.select([scores >= 0.0, scores < 0.0],\n",
        "                        [self.positive_class,\n",
        "                         self.negative_class])\n",
        "        return out\n",
        "\n",
        "    def find_classes(self, Y):\n",
        "        \"\"\"\n",
        "        Finds the set of output classes in the output part Y of the training set.\n",
        "        If there are exactly two classes, one of them is associated to positive\n",
        "        classifier scores, the other one to negative scores. If the number of\n",
        "        classes is not 2, an error is raised.\n",
        "        \"\"\"\n",
        "        classes = sorted(set(Y))\n",
        "        if len(classes) != 2:\n",
        "            raise Exception(\"this does not seem to be a 2-class problem\")\n",
        "        self.positive_class = classes[1]\n",
        "        self.negative_class = classes[0]\n",
        "\n",
        "    def encode_outputs(self, Y):\n",
        "        \"\"\"\n",
        "        A helper function that converts all outputs to +1 or -1.\n",
        "        \"\"\"\n",
        "        return np.array([1 if y == self.positive_class else -1 for y in Y])\n",
        "\n",
        "\n",
        "class LogisticRegression(LinearClassifier):\n",
        "    \"\"\"\n",
        "    A straightforward implementation of the perceptron learning algorithm.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_iter=20, C = 0.01): # C is regularization parameter\n",
        "        \"\"\"\n",
        "        The constructor can optionally take a parameter n_iter specifying how\n",
        "        many times we want to iterate through the training set.\n",
        "        \"\"\"\n",
        "        self.n_iter = n_iter\n",
        "        self.C = C\n",
        "\n",
        "    def fit(self, X, Y):\n",
        "        \"\"\"\n",
        "        Train a linear classifier using the perceptron learning algorithm.\n",
        "        \"\"\"\n",
        "\n",
        "        # First determine which output class will be associated with positive\n",
        "        # and negative scores, respectively.\n",
        "        self.find_classes(Y)\n",
        "\n",
        "        # Convert all outputs to +1 (for the positive class) or -1 (negative).\n",
        "        Ye = self.encode_outputs(Y)\n",
        "\n",
        "        # If necessary, convert the sparse matrix returned by a vectorizer\n",
        "        # into a normal NumPy matrix.\n",
        "        if not isinstance(X, np.ndarray):\n",
        "            X = X.toarray()\n",
        "\n",
        "        # Initialize the weight vector to all zeros.\n",
        "        n_features = X.shape[1]\n",
        "        self.w = np.zeros(n_features) # w = 000 \n",
        "\n",
        "        # Perceptron algorithm:\n",
        "        for i in range(self.n_iter): # repeat \n",
        "            t = 0 \n",
        "            for x, y in zip(X, Ye): # select  a training pair \n",
        "                t = t +1 \n",
        "                eta = 1/(self.C*t)\n",
        "                # Compute the output score for this instance.\n",
        "                score = x.dot(self.w)\n",
        "\n",
        "                # If there was an error, update the weights.\n",
        "                #if y*score <= 0:\n",
        "                #    self.w += y*x\n",
        "                #if y*score < 1: # 0 > (1-y* x.dot(self.w))\n",
        "                self.w =(1-eta*self.C)*self.w + (y/(1+np.exp(y*score)))*x\n",
        "                #else:\n",
        "                    #self.w =(1-eta*self.C)*self.w \n",
        "\n",
        "\n",
        "##### The following part is for the optional task.\n",
        "\n",
        "### Sparse and dense vectors don't collaborate very well in NumPy/SciPy.\n",
        "### Here are two utility functions that help us carry out some vector\n",
        "### operations that we'll need.\n",
        "\n",
        "def add_sparse_to_dense(x, w, factor):\n",
        "    \"\"\"\n",
        "    Adds a sparse vector x, scaled by some factor, to a dense vector.\n",
        "    This can be seen as the equivalent of w += factor * x when x is a dense\n",
        "    vector.\n",
        "    \"\"\"\n",
        "    w[x.indices] += factor * x.data\n",
        "\n",
        "def sparse_dense_dot(x, w):\n",
        "    \"\"\"\n",
        "    Computes the dot product between a sparse vector x and a dense vector w.\n",
        "    \"\"\"\n",
        "    return np.dot(w[x.indices], x.data)\n",
        "\n",
        "\n",
        "class SparsePerceptron(LinearClassifier):\n",
        "    \"\"\"\n",
        "    A straightforward implementation of the perceptron learning algorithm,\n",
        "    assuming that the input feature matrix X is sparse.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_iter=20):\n",
        "        \"\"\"\n",
        "        The constructor can optionally take a parameter n_iter specifying how\n",
        "        many times we want to iterate through the training set.\n",
        "        \"\"\"\n",
        "        self.n_iter = n_iter\n",
        "\n",
        "    def fit(self, X, Y):\n",
        "        \"\"\"\n",
        "        Train a linear classifier using the perceptron learning algorithm.\n",
        "\n",
        "        Note that this will only work if X is a sparse matrix, such as the\n",
        "        output of a scikit-learn vectorizer.\n",
        "        \"\"\"\n",
        "        self.find_classes(Y)\n",
        "\n",
        "        # First determine which output class will be associated with positive\n",
        "        # and negative scores, respectively.\n",
        "        Ye = self.encode_outputs(Y)\n",
        "\n",
        "        # Initialize the weight vector to all zeros.\n",
        "        self.w = np.zeros(X.shape[1])\n",
        "\n",
        "        # Iteration through sparse matrices can be a bit slow, so we first\n",
        "        # prepare this list to speed up iteration.\n",
        "        XY = list(zip(X, Ye))\n",
        "\n",
        "        for i in range(self.n_iter):\n",
        "            for x, y in XY:\n",
        "\n",
        "                # Compute the output score for this instance.\n",
        "                # (This corresponds to score = x.dot(self.w) above.)\n",
        "                score = sparse_dense_dot(x, self.w)\n",
        "\n",
        "                # If there was an error, update the weights.\n",
        "                if y*score <= 0:\n",
        "                    # (This corresponds to self.w += y*x above.)\n",
        "                    add_sparse_to_dense(x, self.w, y)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hB2Bvin3XA90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = make_pipeline(\n",
        "        TfidfVectorizer(),\n",
        "        SelectKBest(k=1000),\n",
        "        Normalizer(),\n",
        "\n",
        "        # NB that this is our Perceptron, not sklearn.linear_model.Perceptron\n",
        "        LogisticRegression(C=1/len(Y))  \n",
        "    )\n",
        "\n",
        "# Train the classifier.\n",
        "t0 = time.time()\n",
        "pipeline.fit(Xtrain, Ytrain)\n",
        "t1 = time.time()\n",
        "print('Training time: {:.2f} sec.'.format(t1-t0))\n",
        "\n",
        "# Evaluate on the test set.\n",
        "Yguess = pipeline.predict(Xtest)\n",
        "print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tm2gcc1feXna",
        "outputId": "afa5df36-9d6f-4b56-9547-273e01fe945d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training time: 4.29 sec.\n",
            "Accuracy: 0.8170.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BONUS 1"
      ],
      "metadata": {
        "id": "NxG9zVCIqHSC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.linalg.blas import ddot,dscal,daxpy"
      ],
      "metadata": {
        "id": "0KcmchR1gNIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"This file shows a couple of implementations of the perceptron learning\n",
        "algorithm. It is based on the code from Lecture 3, but using the slightly\n",
        "more compact perceptron formulation that we saw in Lecture 6.\n",
        "\n",
        "There are two versions: Perceptron, which uses normal NumPy vectors and\n",
        "matrices, and SparsePerceptron, which uses sparse vectors and matrices.\n",
        "The latter may be faster when we have high-dimensional feature representations\n",
        "with a lot of zeros, such as when we are using a \"bag of words\" representation\n",
        "of documents.\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator\n",
        "\n",
        "class LinearClassifier(BaseEstimator):\n",
        "    \"\"\"\n",
        "    General class for binary linear classifiers. Implements the predict\n",
        "    function, which is the same for all binary linear classifiers. There are\n",
        "    also two utility functions.\n",
        "    \"\"\"\n",
        "\n",
        "    def decision_function(self, X):\n",
        "        \"\"\"\n",
        "        Computes the decision function for the inputs X. The inputs are assumed to be\n",
        "        stored in a matrix, where each row contains the features for one\n",
        "        instance.\n",
        "        \"\"\"\n",
        "        return X.dot(self.w)\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predicts the outputs for the inputs X. The inputs are assumed to be\n",
        "        stored in a matrix, where each row contains the features for one\n",
        "        instance.\n",
        "        \"\"\"\n",
        "\n",
        "        # First compute the output scores\n",
        "        scores = self.decision_function(X)\n",
        "\n",
        "        # Select the positive or negative class label, depending on whether\n",
        "        # the score was positive or negative.\n",
        "        out = np.select([scores >= 0.0, scores < 0.0],\n",
        "                        [self.positive_class,\n",
        "                         self.negative_class])\n",
        "        return out\n",
        "\n",
        "    def find_classes(self, Y):\n",
        "        \"\"\"\n",
        "        Finds the set of output classes in the output part Y of the training set.\n",
        "        If there are exactly two classes, one of them is associated to positive\n",
        "        classifier scores, the other one to negative scores. If the number of\n",
        "        classes is not 2, an error is raised.\n",
        "        \"\"\"\n",
        "        classes = sorted(set(Y))\n",
        "        if len(classes) != 2:\n",
        "            raise Exception(\"this does not seem to be a 2-class problem\")\n",
        "        self.positive_class = classes[1]\n",
        "        self.negative_class = classes[0]\n",
        "\n",
        "    def encode_outputs(self, Y):\n",
        "        \"\"\"\n",
        "        A helper function that converts all outputs to +1 or -1.\n",
        "        \"\"\"\n",
        "        return np.array([1 if y == self.positive_class else -1 for y in Y])\n",
        "\n",
        "\n",
        "class LogisticRegressionFaster(LinearClassifier):\n",
        "    \"\"\"\n",
        "    A straightforward implementation of the perceptron learning algorithm.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_iter=20, C = 0.01): # C is regularization parameter\n",
        "        \"\"\"\n",
        "        The constructor can optionally take a parameter n_iter specifying how\n",
        "        many times we want to iterate through the training set.\n",
        "        \"\"\"\n",
        "        self.n_iter = n_iter\n",
        "        self.C = C\n",
        "\n",
        "    def fit(self, X, Y):\n",
        "        \"\"\"\n",
        "        Train a linear classifier using the perceptron learning algorithm.\n",
        "        \"\"\"\n",
        "\n",
        "        # First determine which output class will be associated with positive\n",
        "        # and negative scores, respectively.\n",
        "        self.find_classes(Y)\n",
        "\n",
        "        # Convert all outputs to +1 (for the positive class) or -1 (negative).\n",
        "        Ye = self.encode_outputs(Y)\n",
        "\n",
        "        # If necessary, convert the sparse matrix returned by a vectorizer\n",
        "        # into a normal NumPy matrix.\n",
        "        if not isinstance(X, np.ndarray):\n",
        "            X = X.toarray()\n",
        "\n",
        "        # Initialize the weight vector to all zeros.\n",
        "        n_features = X.shape[1]\n",
        "        self.w = np.zeros(n_features) # w = 000 \n",
        "\n",
        "        # Perceptron algorithm:\n",
        "        for i in range(self.n_iter): # repeat \n",
        "            t = 0 \n",
        "            for x, y in zip(X, Ye): # select  a training pair \n",
        "                t = t +1 \n",
        "                eta = 1/(self.C*t)\n",
        "                # Compute the output score for this instance.\n",
        "                score = ddot(x,self.w)\n",
        "\n",
        "                # If there was an error, update the weights.\n",
        "                #if y*score <= 0:\n",
        "                #    self.w += y*x\n",
        "                #if y*score < 1: # 0 > (1-y* x.dot(self.w))\n",
        "                #self.w =(1-eta*self.C)*self.w + (y/(1+np.exp(y*score)))*x\n",
        "                dscal((1-eta*self.C),self.w) \n",
        "                #self.w  = self.w + (y/(1+np.exp(y*score)))*x\n",
        "\n",
        "                daxpy(x,self.w,a = (y/(1+np.exp(y*score))))\n",
        "                #ddot(y/(1+np.exp(y*score)),x)\n",
        "                #else:\n",
        "                    #self.w =(1-eta*self.C)*self.w \n",
        "\n",
        "\n",
        "##### The following part is for the optional task.\n",
        "\n",
        "### Sparse and dense vectors don't collaborate very well in NumPy/SciPy.\n",
        "### Here are two utility functions that help us carry out some vector\n",
        "### operations that we'll need.\n",
        "\n",
        "def add_sparse_to_dense(x, w, factor):\n",
        "    \"\"\"\n",
        "    Adds a sparse vector x, scaled by some factor, to a dense vector.\n",
        "    This can be seen as the equivalent of w += factor * x when x is a dense\n",
        "    vector.\n",
        "    \"\"\"\n",
        "    w[x.indices] += factor * x.data\n",
        "\n",
        "def sparse_dense_dot(x, w):\n",
        "    \"\"\"\n",
        "    Computes the dot product between a sparse vector x and a dense vector w.\n",
        "    \"\"\"\n",
        "    return np.dot(w[x.indices], x.data)\n",
        "\n",
        "\n",
        "class SparsePerceptron(LinearClassifier):\n",
        "    \"\"\"\n",
        "    A straightforward implementation of the perceptron learning algorithm,\n",
        "    assuming that the input feature matrix X is sparse.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_iter=20):\n",
        "        \"\"\"\n",
        "        The constructor can optionally take a parameter n_iter specifying how\n",
        "        many times we want to iterate through the training set.\n",
        "        \"\"\"\n",
        "        self.n_iter = n_iter\n",
        "\n",
        "    def fit(self, X, Y):\n",
        "        \"\"\"\n",
        "        Train a linear classifier using the perceptron learning algorithm.\n",
        "\n",
        "        Note that this will only work if X is a sparse matrix, such as the\n",
        "        output of a scikit-learn vectorizer.\n",
        "        \"\"\"\n",
        "        self.find_classes(Y)\n",
        "\n",
        "        # First determine which output class will be associated with positive\n",
        "        # and negative scores, respectively.\n",
        "        Ye = self.encode_outputs(Y)\n",
        "\n",
        "        # Initialize the weight vector to all zeros.\n",
        "        self.w = np.zeros(X.shape[1])\n",
        "\n",
        "        # Iteration through sparse matrices can be a bit slow, so we first\n",
        "        # prepare this list to speed up iteration.\n",
        "        XY = list(zip(X, Ye))\n",
        "\n",
        "        for i in range(self.n_iter):\n",
        "            for x, y in XY:\n",
        "\n",
        "                # Compute the output score for this instance.\n",
        "                # (This corresponds to score = x.dot(self.w) above.)\n",
        "                score = sparse_dense_dot(x, self.w)\n",
        "\n",
        "                # If there was an error, update the weights.\n",
        "                if y*score <= 0:\n",
        "                    # (This corresponds to self.w += y*x above.)\n",
        "                    add_sparse_to_dense(x, self.w, y)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "__UqifdjiaWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bonus Faster Implementation"
      ],
      "metadata": {
        "id": "QzbreIKNpahZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bonus- a part"
      ],
      "metadata": {
        "id": "FAxYEgonpd6Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = make_pipeline(\n",
        "        TfidfVectorizer(),\n",
        "        SelectKBest(k=1000),\n",
        "        Normalizer(),\n",
        "\n",
        "        # NB that this is our Perceptron, not sklearn.linear_model.Perceptron\n",
        "        LogisticRegressionFaster(C=1/len(Y))  \n",
        "    )\n",
        "\n",
        "# Train the classifier.\n",
        "t0 = time.time()\n",
        "pipeline.fit(Xtrain, Ytrain)\n",
        "t1 = time.time()\n",
        "print('Training time: {:.2f} sec.'.format(t1-t0))\n",
        "\n",
        "# Evaluate on the test set.\n",
        "Yguess = pipeline.predict(Xtest)\n",
        "print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPfsgvmzjlvW",
        "outputId": "8f0fcdea-e08a-4e50-826c-ca1165223c27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training time: 3.55 sec.\n",
            "Accuracy: 0.8170.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bonus - b part"
      ],
      "metadata": {
        "id": "wLWX-AZYpjH2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = make_pipeline(\n",
        "        TfidfVectorizer(),\n",
        "        Normalizer(),\n",
        "\n",
        "        # NB that this is our Perceptron, not sklearn.linear_model.Perceptron\n",
        "        LogisticRegression(C=1/len(Y))  \n",
        "    )\n",
        "\n",
        "# Train the classifier.\n",
        "t0 = time.time()\n",
        "pipeline.fit(Xtrain, Ytrain)\n",
        "t1 = time.time()\n",
        "print('Training time: {:.2f} sec.'.format(t1-t0))\n",
        "\n",
        "# Evaluate on the test set.\n",
        "Yguess = pipeline.predict(Xtest)\n",
        "print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhYDPeZRpffk",
        "outputId": "3515bda0-ee23-4df9-b138-b4b8f7e642e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training time: 54.57 sec.\n",
            "Accuracy: 0.8133.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "##### The following part is for the optional task.\n",
        "\n",
        "### Sparse and dense vectors don't collaborate very well in NumPy/SciPy.\n",
        "### Here are two utility functions that help us carry out some vector\n",
        "### operations that we'll need.\n",
        "\n",
        "def add_sparse_to_dense(x, w, factor):\n",
        "    \"\"\"\n",
        "    Adds a sparse vector x, scaled by some factor, to a dense vector.\n",
        "    This can be seen as the equivalent of w += factor * x when x is a dense\n",
        "    vector.\n",
        "    \"\"\"\n",
        "    w[x.indices] += factor * x.data\n",
        "\n",
        "def sparse_dense_dot(x, w):\n",
        "    \"\"\"\n",
        "    Computes the dot product between a sparse vector x and a dense vector w.\n",
        "    \"\"\"\n",
        "    return np.dot(w[x.indices], x.data)\n",
        "\n",
        "\n",
        "class SparseLogistic(LinearClassifier):\n",
        "    \"\"\"\n",
        "    A straightforward implementation of the perceptron learning algorithm,\n",
        "    assuming that the input feature matrix X is sparse.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_iter=20, C = 0.01): # C is regularization parameter\n",
        "        \"\"\"\n",
        "        The constructor can optionally take a parameter n_iter specifying how\n",
        "        many times we want to iterate through the training set.\n",
        "        \"\"\"\n",
        "        self.n_iter = n_iter\n",
        "        self.C = C\n",
        "    def fit(self, X, Y):\n",
        "        \"\"\"\n",
        "        Train a linear classifier using the perceptron learning algorithm.\n",
        "\n",
        "        Note that this will only work if X is a sparse matrix, such as the\n",
        "        output of a scikit-learn vectorizer.\n",
        "        \"\"\"\n",
        "        self.find_classes(Y)\n",
        "\n",
        "        # First determine which output class will be associated with positive\n",
        "        # and negative scores, respectively.\n",
        "        Ye = self.encode_outputs(Y)\n",
        "\n",
        "        # Initialize the weight vector to all zeros.\n",
        "        self.w = np.zeros(X.shape[1])\n",
        "\n",
        "        # Iteration through sparse matrices can be a bit slow, so we first\n",
        "        # prepare this list to speed up iteration.\n",
        "        XY = list(zip(X, Ye))\n",
        "\n",
        "        for i in range(self.n_iter): # repeat \n",
        "            t = 0 \n",
        "            for x, y in zip(X, Ye): # select  a training pair \n",
        "                t = t +1 \n",
        "                eta = 1/(self.C*t)\n",
        "                # Compute the output score for this instance.\n",
        "                score = sparse_dense_dot(x,self.w)\n",
        "\n",
        "                # If there was an error, update the weights.\n",
        "                #if y*score <= 0:\n",
        "                #    self.w += y*x\n",
        "                #if y*score < 1: # 0 > (1-y* x.dot(self.w))\n",
        "                self.w =(1-eta*self.C)*self.w \n",
        "                #dscal((1-eta*self.C),self.w) \n",
        "                #self.w  = self.w + (y/(1+np.exp(y*score)))*x\n",
        "\n",
        "                add_sparse_to_dense(x,self.w, (y/(1+np.exp(y*score))))\n",
        "                #ddot(y/(1+np.exp(y*score)),x)\n",
        "                #else:\n",
        "                    #self.w =(1-eta*self.C)*self.w \n"
      ],
      "metadata": {
        "id": "OkHU1S4ErY-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = make_pipeline(\n",
        "        TfidfVectorizer(),\n",
        "        Normalizer(),\n",
        "\n",
        "        # NB that this is our Perceptron, not sklearn.linear_model.Perceptron\n",
        "       SparseLogistic(C=1/len(Y))  \n",
        "    )\n",
        "\n",
        "# Train the classifier.\n",
        "t0 = time.time()\n",
        "pipeline.fit(Xtrain, Ytrain)\n",
        "t1 = time.time()\n",
        "print('Training time: {:.2f} sec.'.format(t1-t0))\n",
        "\n",
        "# Evaluate on the test set.\n",
        "Yguess = pipeline.predict(Xtest)\n",
        "print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8vSq3oer7Le",
        "outputId": "58ebcf4d-2e67-44e0-a682-8d408c78b8bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training time: 393.86 sec.\n",
            "Accuracy: 0.8133.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bonus - c part "
      ],
      "metadata": {
        "id": "CEHN8I7fx1ND"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "##### The following part is for the optional task.\n",
        "\n",
        "### Sparse and dense vectors don't collaborate very well in NumPy/SciPy.\n",
        "### Here are two utility functions that help us carry out some vector\n",
        "### operations that we'll need.\n",
        "\n",
        "def add_sparse_to_dense(x, w, factor):\n",
        "    \"\"\"\n",
        "    Adds a sparse vector x, scaled by some factor, to a dense vector.\n",
        "    This can be seen as the equivalent of w += factor * x when x is a dense\n",
        "    vector.\n",
        "    \"\"\"\n",
        "    w[x.indices] += factor * x.data\n",
        "\n",
        "def sparse_dense_dot(x, w):\n",
        "    \"\"\"\n",
        "    Computes the dot product between a sparse vector x and a dense vector w.\n",
        "    \"\"\"\n",
        "    return np.dot(w[x.indices], x.data)\n",
        "\n",
        "\n",
        "class SparseLogisticC(LinearClassifier):\n",
        "    \"\"\"\n",
        "    A straightforward implementation of the perceptron learning algorithm,\n",
        "    assuming that the input feature matrix X is sparse.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_iter=20, C = 0.01): # C is regularization parameter\n",
        "        \"\"\"\n",
        "        The constructor can optionally take a parameter n_iter specifying how\n",
        "        many times we want to iterate through the training set.\n",
        "        \"\"\"\n",
        "        self.n_iter = n_iter\n",
        "        self.C = C\n",
        "    def fit(self, X, Y):\n",
        "        \"\"\"\n",
        "        Train a linear classifier using the perceptron learning algorithm.\n",
        "\n",
        "        Note that this will only work if X is a sparse matrix, such as the\n",
        "        output of a scikit-learn vectorizer.\n",
        "        \"\"\"\n",
        "        self.find_classes(Y)\n",
        "\n",
        "        # First determine which output class will be associated with positive\n",
        "        # and negative scores, respectively.\n",
        "        Ye = self.encode_outputs(Y)\n",
        "\n",
        "        # Initialize the weight vector to all zeros.\n",
        "        self.w = np.zeros(X.shape[1])\n",
        "\n",
        "        # Iteration through sparse matrices can be a bit slow, so we first\n",
        "        # prepare this list to speed up iteration.\n",
        "        XY = list(zip(X, Ye))\n",
        "\n",
        "        for i in range(self.n_iter): # repeat \n",
        "            t = 0 \n",
        "            a = 1\n",
        "            for x, y in zip(X, Ye): # select  a training pair \n",
        "\n",
        "                t = t +1 \n",
        "                eta = 1/(self.C*t)\n",
        "                a =(1-eta*self.C)*a\n",
        "                score = a*sparse_dense_dot(x,self.w)\n",
        "                #self.w =self.w + x.dot((eta*y/a))\n",
        "                self.w =self.w + x*(eta*y/a)\n",
        "                add_sparse_to_dense(x,self.w, (y/(1+np.exp(y*score))))\n",
        "\n",
        "            self.w = a*self.w \n",
        "\n",
        "\n",
        "pipeline = make_pipeline(\n",
        "        TfidfVectorizer(),\n",
        "        Normalizer(),\n",
        "\n",
        "        # NB that this is our Perceptron, not sklearn.linear_model.Perceptron\n",
        "       SparseLogisticC(C=1/len(Y))  \n",
        "    )\n",
        "\n",
        "# Train the classifier.\n",
        "t0 = time.time()\n",
        "pipeline.fit(Xtrain, Ytrain)\n",
        "t1 = time.time()\n",
        "print('Training time: {:.2f} sec.'.format(t1-t0))\n",
        "\n",
        "# Evaluate on the test set.\n",
        "Yguess = pipeline.predict(Xtest)\n",
        "print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))            "
      ],
      "metadata": {
        "id": "Q66URknpxnAi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "outputId": "ac9c8280-948b-45a3-e6c2-a6f62011b85f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:66: RuntimeWarning: divide by zero encountered in double_scalars\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-87c7159f5fcf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;31m# Train the classifier.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training time: {:.2f} sec.'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mt0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"passthrough\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m                 \u001b[0mfit_params_last_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_params_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_last_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-87c7159f5fcf>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0;31m#self.w =self.w + x.dot((eta*y/a))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meta\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                 \u001b[0madd_sparse_to_dense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-87c7159f5fcf>\u001b[0m in \u001b[0;36madd_sparse_to_dense\u001b[0;34m(x, w, factor)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mvector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \"\"\"\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfactor\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msparse_dense_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/matrixlib/defmatrix.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 30086 is out of bounds for axis 0 with size 1"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = make_pipeline(\n",
        "        TfidfVectorizer(),\n",
        "        Normalizer(),\n",
        "\n",
        "        # NB that this is our Perceptron, not sklearn.linear_model.Perceptron\n",
        "       SparseLogisticC(C=1/len(Y))  \n",
        "    )\n",
        "\n",
        "# Train the classifier.\n",
        "t0 = time.time()\n",
        "pipeline.fit(Xtrain, Ytrain)\n",
        "t1 = time.time()\n",
        "print('Training time: {:.2f} sec.'.format(t1-t0))\n",
        "\n",
        "# Evaluate on the test set.\n",
        "Yguess = pipeline.predict(Xtest)\n",
        "print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "0F9T8MGny5Z3",
        "outputId": "968845ec-9e5d-457c-ee90-b9c3165b7b04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:65: RuntimeWarning: divide by zero encountered in double_scalars\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-d34f6e19a522>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Train the classifier.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training time: {:.2f} sec.'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mt0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"passthrough\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m                 \u001b[0mfit_params_last_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_params_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_last_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-2eef7a038569>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msparse_dense_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meta\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0madd_sparse_to_dense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-2eef7a038569>\u001b[0m in \u001b[0;36madd_sparse_to_dense\u001b[0;34m(x, w, factor)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mvector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \"\"\"\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfactor\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msparse_dense_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/matrixlib/defmatrix.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 30086 is out of bounds for axis 0 with size 1"
          ]
        }
      ]
    }
  ]
}